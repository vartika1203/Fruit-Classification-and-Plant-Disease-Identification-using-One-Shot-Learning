{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmuQ0j443a5W"
      },
      "outputs": [],
      "source": [
        "#importing the libraries\n",
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam\n",
        "from keras import optimizers\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import callbacks\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "import os\n",
        "from keras.models import Model,load_model\n",
        "import json\n",
        "from keras.models import model_from_json, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "chosen_image_size = 100 #image size\n",
        "total_sample_size = 11000 # total number of pairs to be created\n",
        "\n",
        "\n",
        "folder_count = 131 #number of folders or classes\n",
        "image_count = 20 #number of images to be taken from a folder\n",
        "\n",
        "path =  os.path.join('xxxxx') #defining the path of the training dataset\n",
        "print(path)"
      ],
      "metadata": {
        "id": "VB3Nk7pB3hZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the functions we'll need for training\n",
        "def read_image(filename, byteorder='>'):\n",
        "\n",
        "    with open(filename, 'rb') as f:\n",
        "\n",
        "        buffer = f.read()\n",
        "\n",
        "    header, width, height, maxval = re.search(\n",
        "        b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n",
        "\n",
        "    return np.frombuffer(buffer,\n",
        "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
        "                            count=int(width)*int(height),\n",
        "                            offset=len(header)\n",
        "                            ).reshape((int(height), int(width)))\n",
        "\n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return labels[predictions.ravel() < 0.5].mean()\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n"
      ],
      "metadata": {
        "id": "tIVMTW9i3lX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "img=plt.imread(path+'S1/1_100.jpg')"
      ],
      "metadata": {
        "id": "OkBxc_fH4ASj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the data and making the pairs\n",
        "import numpy as np\n",
        "def get_data(size, total_sample_size):\n",
        "    img = plt.imread('xxx') #reading the image path\n",
        "    if resize == True:\n",
        "        image = image[::size, ::size]\n",
        "    dim1 = image.shape[0]\n",
        "    dim2 = image.shape[1]\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
        "    x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n",
        "\n",
        "    y_genuine = np.zeros([total_sample_size,1])\n",
        "\n",
        "    for i in range(folder_count):\n",
        "        for j in range(int(total_sample_size/folder_count)):\n",
        "            ind1 = 0\n",
        "            ind2 = 0\n",
        "\n",
        "            #read images from same directory (genuine pair)\n",
        "            while ind1 == ind2:\n",
        "                ind1 = np.random.randint(image_count)\n",
        "                ind2 = np.random.randint(image_count)\n",
        "\n",
        "            # read the two images\n",
        "            image1 = plt.imread(path+'S' + str(i+1) + '/' + str(ind1 + 1) + '_100.jpg', 'rw+')\n",
        "\n",
        "            image2 = plt.imread(path+'S' + str(i+1) + '/' + str(ind2 + 1) + '_100.jpg', 'rw+')\n",
        "\n",
        "\n",
        "            #reduce the size\n",
        "            if resize == True:\n",
        "                img1 = img1[::size, ::size]\n",
        "                img2 = img2[::size, ::size]\n",
        "\n",
        "            #store the images to the initialized numpy array\n",
        "            print\n",
        "            x_geuine_pair[count, 0, 0, :, :] = img1\n",
        "            x_geuine_pair[count, 1, 0, :, :] = img2\n",
        "\n",
        "            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n",
        "            y_genuine[count] = 1\n",
        "            count += 1\n",
        "\n",
        "    count = 0\n",
        "    x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n",
        "    y_imposite = np.zeros([total_sample_size, 1])\n",
        "\n",
        "    for i in range(int(total_sample_size/image_count)):\n",
        "        for j in range(image_count):\n",
        "\n",
        "            #read images from different directory (imposite pair)\n",
        "            while True:\n",
        "                ind1 = np.random.randint(folder_count)\n",
        "                ind2 = np.random.randint(folder_count)\n",
        "                if ind1 != ind2:\n",
        "                    break\n",
        "            image1 = plt.imread(path+'S' + str(ind1+1) + '/' + str(j + 1) + '_100.jpg', 'rw+')\n",
        "\n",
        "            image2 = plt.imread(path+'S' + str(ind2+1) + '/' + str(j + 1) + '_100.jpg', 'rw+')\n",
        "\n",
        "            if resize == True:\n",
        "                img1 = img1[::size, ::size]\n",
        "                img2 = img2[::size, ::size]\n",
        "\n",
        "            x_imposite_pair[count, 0, 0, :, :] = img1\n",
        "            x_imposite_pair[count, 1, 0, :, :] = img2\n",
        "            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n",
        "            y_imposite[count] = 0\n",
        "            count += 1\n",
        "\n",
        "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
        "    #print(x_geuine_pair.shape)\n",
        "    #print(x_imposite_pair.shape)\n",
        "    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255\n",
        "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
        "\n",
        "    return X, Y\n",
        "X, Y = get_data(size, total_sample_size)"
      ],
      "metadata": {
        "id": "Fb2MOjdU4Jwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting a plot\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(Y[:,0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PXfZcjaJ4-YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a sequential model which serves as a sequential cnn in siamese network and returns a feature vector\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "def build_base_network(input_shape):\n",
        "\n",
        "    seq = tf.keras.Sequential()\n",
        "    nb_filter = [16, 32, 16]\n",
        "    kernel_size = 2\n",
        "    seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape,padding='valid'))\n",
        "    seq.add(Activation('relu'))\n",
        "    seq.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    seq.add(Dropout(.25))\n",
        "    seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, padding='valid'))\n",
        "    seq.add(Activation('relu'))\n",
        "    seq.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    seq.add(Dropout(.25))\n",
        "    seq.add(Convolution2D(nb_filter[2], kernel_size, kernel_size, padding='valid'))\n",
        "    seq.add(Activation('relu'))\n",
        "    seq.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    seq.add(Dropout(.25)\n",
        "    seq.add(Flatten())\n",
        "    seq.add(Dense(128, activation='relu'))\n",
        "    seq.add(Dropout(0.1))\n",
        "    seq.add(Dense(50, activation='relu'))\n",
        "    return seq"
      ],
      "metadata": {
        "id": "GgAbxwx15FxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X.shape[2:]\n",
        "img_a = Input(shape=input_dim)\n",
        "img_b = Input(shape=input_dim)"
      ],
      "metadata": {
        "id": "McgEEmWN5Sqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_network = build_base_network(input_dim)\n",
        "feat_vecs_a = base_network(img_a)\n",
        "feat_vecs_b = base_network(img_b)"
      ],
      "metadata": {
        "id": "YI1PJ8Lj5U33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the distance layer\n",
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])"
      ],
      "metadata": {
        "id": "VkpG_Qto5Yp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining optimizer\n",
        "epochs = 20\n",
        "rms = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#RMSprop()\n",
        "rms = RMSprop()"
      ],
      "metadata": {
        "id": "83tpsb7k5dSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definign the log and model directory\n",
        "LogDir=\"xxxxx\"\n",
        "ModelDir=\"yyyyy\""
      ],
      "metadata": {
        "id": "JlyX-Xxi5ly3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "from tensorflow import keras\n",
        "csv_logger = keras.callbacks.CSVLogger(LogDir + \"/\" + LogPath + \"-acc.csv\", append = True)\n",
        "\n",
        "entirecheckpointLast = keras.callbacks.ModelCheckpoint(filepath = ModelDir + \"/\" + LogPath + \"-entire-last.h5\", verbose = 0)\n",
        "\n",
        "entirecheckpointBest = keras.callbacks.ModelCheckpoint(filepath = ModelDir + \"/\" + LogPath + \"-entire-best.h5\", verbose = 1, save_best_only = True)"
      ],
      "metadata": {
        "id": "0pOjL-P15ozO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compiling the model\n",
        "model = Model([img_a, img_b], distance)\n",
        "model.compile(loss=contrastive_loss, optimizer=rms,metrics=[accuracy])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "37uB3CYZ5reU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "img_1 = X[:, 0]\n",
        "img2 = X[:, 1]\n",
        "img_1.shape\n",
        "history = model.fit([img_1, img2], Y, validation_split=.1,\n",
        "      batch_size= batch_size, verbose=1,epochs=epochs,callbacks = [csv_logger, entirecheckpointLast, entirecheckpointBest,callback_early_stop_reduceLROnPlateau])\n"
      ],
      "metadata": {
        "id": "_oM6wI-E50No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the model\n",
        "model.save_weights('one_shot_1_2.h5')\n",
        "with open('model_architecture_1_2.json', 'w') as f:\n",
        "    f.write(model.to_json())\n",
        "print('saved')"
      ],
      "metadata": {
        "id": "eP6Fs8M0531I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}